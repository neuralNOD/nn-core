{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = \"center\">Univariate LSTM Network</h1>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Objective:** Create a *bare neural network* LSTM model for univariate time series data and check the functionalities and capabilities. Note the model developed here are just for informational purpose and the actual model is to be developed and trained in cloud. The notebook also serves as a reference to all the *beautiful* custom-built user-defined functionalities available.\n",
    "\n",
    "## Prediction of Future Sequence\n",
    "\n",
    "A LSTM network can be developed to predict future sequence of a given length (defined as **`n_forecast`**) by - (I) creating a output layer (generally `Dense`) of `length == n_forecast`, as described [here](https://stackoverflow.com/a/69912334/6623589), or (II) create a function that takes previous input and keep on iterating for `n_forecast` as in [this video](https://youtu.be/UbvkhuqVqUI?t=1026). In this notebook, both the approach is tried however the accuracy metric and performance comparison are to be added later. The `n_lookup` and `n_forecast` can be explained as in:\n",
    "\n",
    "![prediction-sequence](https://i.stack.imgur.com/YXwMJ.png)\n",
    "\n",
    "where, $T$ is the lookback period defined as **`n_lookup`** throught the code, and $H$ is the forecast period defined as **`n_forecast`**.\n",
    "\n",
    "### Lookback Period (`n_lookup`)\n",
    "\n",
    "The day's considered while model training. Based on *previous analysis* it is observed that past fifteen (15) days data has an impact on the bid execution day (i.e. $D_{t_{1}}$) thus for model training we can cosider a sequence of shape `(-1, 15 * 96, 1)` where `96` is the number of blocks for a day. Thus, `n_lookup` will take a sequence data comprising of $D_{t_{-16}}$ to $D_{t_{-2}}$ while sitting on $D_{t_{-1}}$ to predict.\n",
    "\n",
    "### Forecast Period (`n_forecast`)\n",
    "\n",
    "Sitting on $D_{t_{-1}}$ to predict, the bids are to be placed on $D_{t_{0}}$ for $D_{t_{1}}$ thus we need `2 * 96 = 192` worth sequence of data. While some data may already available, this needs more clarifications and thus total data of $D_{t_{0}}$ is neglected for all markets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T10:27:05.322993Z",
     "start_time": "2023-01-13T10:27:05.309779Z"
    }
   },
   "outputs": [],
   "source": [
    "# use the code release version for tracking and code modifications. use the\n",
    "# CHANGELOG.md file to keep track of version features, and/or release notes.\n",
    "# the version file is avaiable at project root directory, check the\n",
    "# global configuration setting for root directory information.\n",
    "# the file is already read and is available as `__version__`\n",
    "__version__ = open(\"../VERSION\", \"rt\").read() # bump codecov\n",
    "print(f\"Current Code Version: {__version__}\") # TODO : author, contact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Imports\n",
    "\n",
    "A code must be written such that it is always _production ready_. The conventional guidelines provided under [**PEP8**](https://peps.python.org/pep-0008/#imports) defines the conventional or syntactically useful ways of defining and/or manipulating functions. Necessar guidelines w.r.t. code imports are mentioned below, and basic libraries and import settings are defined.\n",
    "\n",
    " 1. Imports should be on separate lines,\n",
    " 2. Import order should be:\n",
    "    * standard library/modules,\n",
    "    * related third party imports,\n",
    "    * local application/user defined imports\n",
    " 3. Wildcard import (`*`) should be avoided, else specifically tagged with **`# noqa: F403`** as per `flake8` or **`# pylint: disable=unused-import`** as per `pylint`\n",
    " 4. Avoid using relative imports; use explicit imports instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T10:27:05.339011Z",
     "start_time": "2023-01-13T10:27:05.326014Z"
    }
   },
   "outputs": [],
   "source": [
    "import os   # miscellaneous os interfaces\n",
    "import sys  # configuring python runtime environment\n",
    "import time # library for time manipulation, and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T10:27:05.355015Z",
     "start_time": "2023-01-13T10:27:05.341011Z"
    }
   },
   "outputs": [],
   "source": [
    "# use `datetime` to control and preceive the environment\n",
    "# in addition `pandas` also provides date time functionalities\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T10:27:05.370041Z",
     "start_time": "2023-01-13T10:27:05.358014Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy      # dataframe is mutable\n",
    "# from tqdm import tqdm as TQ    # progress bar for loops\n",
    "# from uuid import uuid4 as UUID # unique identifier for objs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`logging`**](https://docs.python.org/3/howto/logging.html) is a standard python module that is meant for tracking any events that happen during any software/code operations. This module is super powerful and helpful for code debugging and other purposes. The next section defines a `logging` configuration in **`../logs/`** directory. Modify the **`LOGS_DIR`** variable under *Global Arguments* to change the default directory. The module is configured with a simplistic approach, such that any `print())` statement can be update to `logging.LEVEL_NAME()` and the code will work. Use logging operations like:\n",
    "\n",
    "```python\n",
    " >> logging.debug(\"This is a Debug Message.\")\n",
    " >> logging.info(\"This is a Information Message.\")\n",
    " >> logging.warning(\"This is a Warning Message.\")\n",
    " >> logging.error(\"This is a ERROR Message.\")\n",
    " >> logging.critical(\"This is a CRITICAL Message.\")\n",
    "```\n",
    "\n",
    "Note: some directories related to logging is created by default. This can be updated/changed in the following configuration section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T11:55:57.663629Z",
     "start_time": "2022-05-07T11:55:57.657630Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import logging # configure logging on `global arguments` section, as file path is required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis and AI/ML Libraries\n",
    "\n",
    "Import of data analysis and AI/ML libraries required at different intersections. Check settings and configurations [here](https://gitlab.com/ZenithClown/computer-configurations-and-setups) and code snippets [here](https://gitlab.com/ZenithClown/computer-configurations-and-setups/-/tree/master/template/snippets/vscode) for understanding settings that is used in this notebook. The code uses `matplotlib.styles` which is a custom `.mplstyle` file recognised by the `matplotlib` downlodable from [this link](https://gitlab.com/ZenithClown/computer-configurations-and-setups/-/tree/master/settings/python/matplotlib)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T10:27:08.138875Z",
     "start_time": "2023-01-13T10:27:05.372014Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%precision 3\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid');\n",
    "plt.style.use('default-style');\n",
    "\n",
    "pd.set_option('display.max_rows', 50) # max. rows to show\n",
    "pd.set_option('display.max_columns', 15) # max. cols to show\n",
    "np.set_printoptions(precision = 3, threshold = 15) # set np options\n",
    "pd.options.display.float_format = '{:,.3f}'.format # float precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T10:27:08.375878Z",
     "start_time": "2023-01-13T10:27:08.140769Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# for rmse, use `squared = False` : https://stackoverflow.com/a/18623635/\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error as MSE,\n",
    "    mean_absolute_error as MAE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T10:27:14.897190Z",
     "start_time": "2023-01-13T10:27:08.378892Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(f\"Tensorflow Version: {tf.__version__}\", end = \"\\n\\n\") # required >= 2.8\n",
    "\n",
    "# check physical devices, and gpu compute capability (if available)\n",
    "if len(tf.config.list_physical_devices(device_type = \"GPU\")):\n",
    "    # https://stackoverflow.com/q/38009682/6623589\n",
    "    # https://stackoverflow.com/a/59179238/6623589\n",
    "    print(\"GPU Computing Available.\", end = \" \")\n",
    "    \n",
    "    # experimentally, get the gpu details and computation power\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/config/experimental/get_device_details\n",
    "    devices = tf.config.list_physical_devices(device_type = \"GPU\")[0] # first\n",
    "    details = tf.config.experimental.get_device_details(devices) # only first\n",
    "    details.get('device_name', 'compute_capability')\n",
    "    print(f\"EXPERIMENTAL : {details}\")\n",
    "else:\n",
    "    print(\"GPU Computing Not Available. If `GPU` is present, check configuration. Detected Devices:\")\n",
    "    print(\"  > \", tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Function(s)\n",
    "\n",
    "It is recommended that any UDFs are defined outside the scope of the *jupyter notebook* such that development/editing of function can be done more practically. As per *programming guidelines* as [`src`](https://fileinfo.com/extension/src) file/directory is beneficial in code development and/or production release. However, *jupyter notebook* requires *kernel restart* if any imported code file is changed in disc, for this frequently changing functions can be defined in this section.\n",
    "\n",
    "**Getting Started** with **`PYTHONPATH`**\n",
    "\n",
    "One must know what are [Environment Variable](https://medium.com/chingu/an-introduction-to-environment-variables-and-how-to-use-them-f602f66d15fa) and how to call/use them in your choice of programming language. Note that an environment variable is *case sensitive* in all operating systems (except windows, since DOS is not case sensitive). Generally, we can access environment variables from terminal/shell/command prompt as:\n",
    "\n",
    "```shell\n",
    "# macOS/*nix\n",
    "echo $VARNAME\n",
    "\n",
    "# windows\n",
    "echo %VARNAME%\n",
    "```\n",
    "\n",
    "Once you've setup your system with [`PYTHONPATH`](https://bic-berkeley.github.io/psych-214-fall-2016/using_pythonpath.html) as per [*python documentation*](https://docs.python.org/3/using/cmdline.html#envvar-PYTHONPATH) is an important directory where any `import` statements looks for based on their order of importance. If a source code/module is not available check necessary environment variables and/or ask the administrator for the source files. For testing purpose, the module boasts the use of `src`, `utils` and `config` directories. However, these directories are available at `ROOT` level, and thus using `sys.path.append()` to add directories while importing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T10:27:14.913173Z",
     "start_time": "2023-01-13T10:27:14.900172Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# append `src` and sub-modules to call additional files these directory are\n",
    "# project specific and not to be added under environment or $PATH variable\n",
    "sys.path.append(os.path.join(\"..\", \"src\")) # parent/source files directory\n",
    "sys.path.append(os.path.join(\"..\", \"src\", \"agents\")) # agents for reinforcement modelling\n",
    "sys.path.append(os.path.join(\"..\", \"src\", \"engine\")) # derivative engines for model control\n",
    "sys.path.append(os.path.join(\"..\", \"src\", \"models\")) # actual models for decision making tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T10:27:14.929174Z",
     "start_time": "2023-01-13T10:27:14.915170Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# also append the `utilities` directory for additional helpful codes\n",
    "sys.path.append(os.path.join(\"..\", \"utilities\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T10:27:14.960324Z",
     "start_time": "2023-01-13T10:27:14.935175Z"
    }
   },
   "outputs": [],
   "source": [
    "from plotting import * # noqd: F403 # pylint: disable=unused-import\n",
    "\n",
    "from trainer import base\n",
    "from lstm import BareLSTM\n",
    "from featuring import CreateSequence\n",
    "from scaler import UnivariateRangedScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Argument(s)\n",
    "\n",
    "The global arguments are *notebook* specific, however they may also be extended to external libraries and functions on import. The *boilerplate* provides a basic ML directory structure which contains a directory for `data` and a separate directory for `output`. In addition, a separate directory (`data/processed`) is created to save processed dataset such that preprocessing can be avoided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T12:02:30.668471Z",
     "start_time": "2022-05-07T12:02:30.661377Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "ROOT = \"..\" # the document root is one level up, that contains all code structure\n",
    "DATA = join(ROOT, \"data\") # the directory contains all data files, subdirectory (if any) can also be used/defined\n",
    "\n",
    "# processed data directory can be used, such that preprocessing steps is not\n",
    "# required to run again-and-again each time on kernel restart\n",
    "PROCESSED_DATA = join(DATA, \"processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T09:15:44.517510Z",
     "start_time": "2023-01-13T09:15:44.503499Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# long projects can be overwhelming, and keeping track of files, outputs and\n",
    "# saved models can be intriguing! to help this out, `today` can be used. for\n",
    "# instance output can be stored at `output/<today>/` etc.\n",
    "# `today` is so configured that it permits windows/*.nix file/directory names\n",
    "today = dt.datetime.strftime(dt.datetime.strptime(time.ctime(), \"%a %b %d %H:%M:%S %Y\"), \"%a, %b %d %Y\")\n",
    "print(f\"Code Execution Started on: {today}\") # only date, name of the sub-directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T12:07:25.311475Z",
     "start_time": "2022-05-07T12:07:25.297545Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = join(ROOT, \"output\", today)\n",
    "# makedirs(OUTPUT_DIR, exist_ok = True) # create dir if not exist\n",
    "\n",
    "# also create directory for `logs`\n",
    "# LOGS_DIR = join(ROOT, \"logs\", open(\"../VERSION\", 'rt').read())\n",
    "# makedirs(LOGS_DIR, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T12:07:35.345557Z",
     "start_time": "2022-05-07T12:07:35.334528Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    filename = join(LOGS_DIR, f\"{today}.log\"), # change `reports` file name\n",
    "    filemode = \"a\", # append logs to existing file, if file exists\n",
    "    format = \"%(asctime)s - %(name)s - CLASS:%(levelname)s:%(levelno)s:L#%(lineno)d - %(message)s\",\n",
    "    level = logging.DEBUG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Input File(s)\n",
    "\n",
    "A typical machine learning project revolves around six important stages (as available in [Amazon ML Life Cycle Documentation](https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/well-architected-machine-learning-lifecycle.html)). The notebook boilerplate is provided to address two pillars:\n",
    "\n",
    " 1. **Data Processing:** An integral part of any machine learning project, which is the most time consuming step! A brief introduction and best practices is available [here](https://towardsdatascience.com/introduction-to-data-preprocessing-in-machine-learning-a9fa83a5dc9d).\n",
    " 2. **Model Development:** From understanding to deployment, this section address development (training, validating and testing) of an machine learning model.\n",
    "\n",
    "![ML Life Cycle](https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/images/ml-lifecycle.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T11:30:38.090716Z",
     "start_time": "2023-01-13T11:30:08.990678Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "MARKET_SNAPSHOT_FILE_PATH = r\"E:\\database\\Indian Energy Exchange\\Market Data - Day Ahead Market (DAM)\\PROCESSED_MarketSnapshot_01-04-2012_31-12-2022.xlsx\"\n",
    "market_snapshot = pd.read_excel(MARKET_SNAPSHOT_FILE_PATH, sheet_name = \"MarketSnapshot\")\n",
    "market_snapshot[\"EffectiveDate\"] = pd.to_datetime(market_snapshot[\"EffectiveDate\"], format = \"%Y-%m-%d\")\n",
    "\n",
    "# already known that 01-08-2012 data records are missing from data source, and\n",
    "# this is a very old record, thus just copy paste the previous days records\n",
    "missing_records = deepcopy(market_snapshot[market_snapshot[\"EffectiveDate\"] == dt.datetime(year = 2022, month = 7, day = 31)])\n",
    "missing_records[\"EffectiveDate\"] = pd.Timestamp(year = 2022, month = 7, day = 31)\n",
    "\n",
    "market_snapshot = pd.concat([market_snapshot, missing_records], ignore_index = True)\n",
    "market_snapshot.sort_values(by = [\"EffectiveDate\", \"BlockID\"], inplace = True)\n",
    "\n",
    "# insert additional columns like year, month and day\n",
    "market_snapshot[\"year\"], market_snapshot[\"month\"], market_snapshot[\"day\"] = zip(*market_snapshot[\"EffectiveDate\"].apply(lambda x : (x.year, x.month, x.day)))\n",
    "market_snapshot = market_snapshot[[\"EffectiveDate\", \"year\", \"month\", \"day\", \"BlockID\", \"PurchaseBid\", \"SellBid\", \"MCV\", \"MCP\"]]\n",
    "\n",
    "market_snapshot.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)\n",
    "\n",
    "Extensive EDA has been *previously* discussed, however though different patterns were available the same does not give a conclusive story that defines the **MCP (Market Clearing Price)** of a given block. In addition, reasons for sudden shocks is abrupt and is not fully explained from the *market snapshot* data. Keeping the core logic at minimum, the following section analyze the data and gives some aggregated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T11:50:31.694353Z",
     "start_time": "2023-01-13T11:50:31.504340Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# create a monthly aggregated statistical summary sheet\n",
    "monthly_statistics_ = market_snapshot.drop(columns = [\"EffectiveDate\", \"day\", \"BlockID\"]) \\\n",
    "    .groupby(by = [\"year\", \"month\"]) \\\n",
    "    .agg([\n",
    "        \"min\",\n",
    "        lambda x : np.quantile(x, 0.05),\n",
    "        lambda x : np.quantile(x, 0.25),\n",
    "        lambda x : np.quantile(x, 0.50),\n",
    "        lambda x : np.quantile(x, 0.75),\n",
    "        lambda x : np.quantile(x, 0.95),\n",
    "        \"max\"\n",
    "    ]) \\\n",
    "    .rename(columns = {\n",
    "        \"<lambda_0>\" : \"q05\",\n",
    "        \"<lambda_1>\" : \"q25\",\n",
    "        \"<lambda_2>\" : \"q50\",\n",
    "        \"<lambda_3>\" : \"q75\",\n",
    "        \"<lambda_4>\" : \"q95\",\n",
    "    })\n",
    "\n",
    "monthly_statistics_.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "lineplot_1d(\n",
    "    monthly_statistics_[[(\"SellBid\", \"min\"), (\"PurchaseBid\", \"min\")]].values,\n",
    "    xticks = list(map(lambda dt : f\"{dt[0]}-{str(dt[1]).zfill(2)}\", monthly_statistics_.index.values)),\n",
    "    legends = [\"Sell Bid\", \"Purchase Bid\"],\n",
    "    xlabel = \"Effective Date $\\longrightarrow$\",\n",
    "    ylabel = \"Bid Amount (in MW) $\\longrightarrow$\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scaling\n",
    "\n",
    "Currently, let's define a basic model considering only a univariate time series data of *market clearing price* for which we scale the price considering the minimum and maximum allowed price for that particular day. Interestingly, minimum price has always been ₹ 0.10 / MW while the maximum price is:\n",
    "\n",
    "$$\n",
    "p_b \\in\n",
    "\\begin{cases}\n",
    "    ₹ 20.00, \\text{upto 03.04.2022} \\\\\n",
    "    ₹ 12.00, \\text{from 04.04.2022}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Considering the above use case in mind, the `MinMaxScaler` has been tweaked to accept `x_min` and `x_max` parameters, while working with a univariate time series data like this. Check documentation using **`help(UnivariateRangedScaler)`** for more information. The general scaling formula used as:\n",
    "\n",
    "$$\n",
    "    \\hat{x} = \\tau_0 + \\frac{x - x_{min}}{x_{max} - x_{min}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T11:34:17.352273Z",
     "start_time": "2023-01-13T11:34:17.277173Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "scaler0 = UnivariateRangedScaler(x_min = 0.10 * 1e3, x_max = 20.00 * 1e3, feature_range = (1, 2))\n",
    "mcp_values_0 = market_snapshot[market_snapshot[\"EffectiveDate\"] <= dt.datetime(year = 2022, month = 4, day = 3)][\"MCP\"].values\n",
    "sc_mcp_values_0 = scaler0.fit_transform(mcp_values_0)\n",
    "\n",
    "scaler1 = UnivariateRangedScaler(x_min = 0.10 * 1e3, x_max = 12.00 * 1e3, feature_range = (1, 2))\n",
    "mcp_values_1 = market_snapshot[market_snapshot[\"EffectiveDate\"] >= dt.datetime(year = 2022, month = 4, day = 4)][\"MCP\"].values\n",
    "sc_mcp_values_1 = scaler1.fit_transform(mcp_values_1)\n",
    "\n",
    "market_snapshot[\"scaled(MCP)\"] = np.concatenate((sc_mcp_values_0, sc_mcp_values_1))\n",
    "market_snapshot.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating `data` Variable\n",
    "\n",
    "For small case approach, consider only ten continuous days of data and extract the `y` matrix as an `np.ndarray` for further processing. To keep memory at *minimum* also delete all table and data variables. 🚧 TODO create a `pkl` file to load and perform analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T10:28:04.086371Z",
     "start_time": "2023-01-13T10:28:04.073372Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "START_DATE = dt.datetime(year = 2021, month = 5, day = 1)\n",
    "FINAL_DATE = dt.datetime(year = 2021, month = 5, day = 10)\n",
    "\n",
    "data = market_snapshot[(market_snapshot[\"EffectiveDate\"] >= START_DATE) & (market_snapshot[\"EffectiveDate\"] <= FINAL_DATE)]\n",
    "data = data[\"scaled(MCP)\"].values\n",
    "\n",
    "# assert data integrity, and also check if all records are present\n",
    "n_ = (FINAL_DATE - START_DATE).days + 1 # no. of days of records\n",
    "assert data.shape[0] == n_ * 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T10:28:04.102367Z",
     "start_time": "2023-01-13T10:28:04.088368Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "del market_snapshot, mcp_values_0, sc_mcp_values_0, mcp_values_1, sc_mcp_values_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating `test` Variable\n",
    "\n",
    "Considering the bare minimal approach, the model will take `lookup_days` of data and give output for `forecast_days`, which is taken as model input to check and develop model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T11:34:21.921788Z",
     "start_time": "2023-01-13T11:34:21.901792Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "TEST_START_DAY = dt.datetime(year = 2021, month = 5, day = 5)\n",
    "TEST_FINAL_DAY = dt.datetime(year = 2021, month = 5, day = 7)\n",
    "\n",
    "PREDICTION_START_DAY = dt.datetime(year = 2021, month = 5, day = 8)\n",
    "PREDICTION_FINAL_DAY = dt.datetime(year = 2021, month = 5, day = 9)\n",
    "\n",
    "test_data = market_snapshot[(market_snapshot[\"EffectiveDate\"] >= TEST_START_DAY) & (market_snapshot[\"EffectiveDate\"] <= TEST_FINAL_DAY)][\"scaled(MCP)\"].values\n",
    "y_actual_ = market_snapshot[(market_snapshot[\"EffectiveDate\"] >= PREDICTION_START_DAY) & (market_snapshot[\"EffectiveDate\"] <= PREDICTION_FINAL_DAY)][\"scaled(MCP)\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T10:28:04.117041Z",
     "start_time": "2023-01-13T10:28:04.104366Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "n_lookback = 3 * 96 # 🧪 look into 3 days past records\n",
    "n_forecast = 2 * 96 # on t(-1) we want prediction for t(+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T10:28:04.133023Z",
     "start_time": "2023-01-13T10:28:04.119020Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "x_train, y_train = CreateSequence(data).create_univariate_series(\n",
    "    n_lookback = n_lookback,\n",
    "    n_forecast = n_forecast\n",
    ")\n",
    "\n",
    "# assert data shapes, which should always match\n",
    "assert x_train.shape[1] == n_lookback\n",
    "assert y_train.shape[1] == n_forecast\n",
    "\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development\n",
    "\n",
    "The model is developed and trained using *user-defined* functions available typically under **`src`**, which makes it easier to keep all the codes and functionalities same, and just change the model and input to suit the need and easily switch between R&D, testing and production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T10:28:04.149019Z",
     "start_time": "2023-01-13T10:28:04.135021Z"
    }
   },
   "outputs": [],
   "source": [
    "# neural network parameters, parametric as much possible\n",
    "ACTIVATION_FUNCTION = \"relu\"\n",
    "\n",
    "# model tuning parameters\n",
    "LR_START = 1e-3\n",
    "LR_FINAL = 2e-4\n",
    "NUM_EPOCHS = 51\n",
    "# BATCH_SIZE = 1024\n",
    "\n",
    "# callback and model monitoring criteria\n",
    "LR_FUNC = tf.keras.callbacks.ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.5, patience = 4)\n",
    "ES_FUNC = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 12, min_delta = 0.001, restore_best_weights = True)\n",
    "TM_FUNC = tf.keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "# define the callbacks for model\n",
    "callbacks = [\n",
    "    # LR_FUNC, # learning rate\n",
    "    ES_FUNC, # early stopping of model training\n",
    "    # TM_FUNC  # terminate model training on null value\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T10:28:05.342912Z",
     "start_time": "2023-01-13T10:28:04.152018Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# https://ai.stackexchange.com/a/3162\n",
    "# https://stackoverflow.com/a/59073430\n",
    "# https://stackoverflow.com/a/58868383\n",
    "# https://datascience.stackexchange.com/a/65079\n",
    "# https://datascience.stackexchange.com/a/18049\n",
    "# https://datascience.stackexchange.com/q/12964\n",
    "# conf. paper (word cloud proj.) https://ieeexplore.ieee.org/document/9132839\n",
    "# tensorflow layer not using cuDNN https://stackoverflow.com/q/68844792/6623589\n",
    "\n",
    "model = BareLSTM(\n",
    "    input_shape = (n_lookback, 1), # ⚠ for univariate shape is always `(-1, 1)`\n",
    "    output_shape = n_forecast, # 💿 high network will throw resource error\n",
    "    activation = ACTIVATION_FUNCTION\n",
    ").get_2_layer_lstm(units = [128, 32])\n",
    "model.summary(line_length = 127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T10:30:01.572226Z",
     "start_time": "2023-01-13T10:28:05.346913Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "trainer = base(model)\n",
    "trainer.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = LR_START, amsgrad = True),\n",
    "    loss = tf.keras.losses.MeanSquaredError(name = \"loss\"),\n",
    "    metrics = [tf.keras.metrics.RootMeanSquaredError(name = \"RMSE\"), tf.keras.metrics.MeanAbsoluteError(name = \"MAE\")]\n",
    ")\n",
    "\n",
    "# get the history from `.train` method\n",
    "history = trainer.train(x = x_train, y = y_train, num_epochs = NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T10:30:01.917228Z",
     "start_time": "2023-01-13T10:30:01.573229Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (27, 3))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history[\"loss\"], label = \"loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label = \"val_loss\")\n",
    "\n",
    "plt.title(\"Loss Metric (AUC)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history[\"MAE\"], label = \"MAE\")\n",
    "plt.plot(history.history[\"val_MAE\"], label = \"val_MAE\")\n",
    "\n",
    "plt.title(\"Mean Absolute Error (MAE)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T11:38:52.844396Z",
     "start_time": "2023-01-13T11:38:52.485898Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "y_predicted = trainer.model.predict(test_data.reshape(-1, n_lookback, 1))[0]\n",
    "\n",
    "# calculate the mae and rmse loss for the model as:\n",
    "print(f\"\"\"\n",
    "Error Analysis Report\n",
    "=====================\n",
    "  Mean Absolute Error (MAE)      : {MAE(y_actual_, y_predicted):.5f}\n",
    "  Root Mean Squared Error (RMSE) : {MSE(y_actual_, y_predicted, squared = False):.5f}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T11:39:31.242125Z",
     "start_time": "2023-01-13T11:39:31.007144Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (27, 3))\n",
    "\n",
    "plt.plot(y_actual_, label = \"$y$\")\n",
    "plt.plot(y_predicted, label = \"$\\hat{y}$\")\n",
    "\n",
    "plt.xticks([]) # close for now\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "TensorFlow 2.9.0 (GPU)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
